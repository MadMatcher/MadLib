{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43e6cf0",
   "metadata": {},
   "source": [
    "# MadLib: Comprehensive Usage Examples\n",
    "\n",
    "This notebook provides an end-to-end demonstration of all public API functions and abstract classes in the MadLib toolkit. Each section includes explanations, code examples, and output displays for different input types and configurations.\n",
    "\n",
    "## What is MadLib?\n",
    "\n",
    "MadLib is a toolkit for the matching step of entity matching that provides:\n",
    "- Multiple tokenization strategies for text preprocessing\n",
    "- Various similarity functions for comparing records\n",
    "- Active learning support for efficient labeling\n",
    "- Extensible abstract classes for custom implementations\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#Setup)\n",
    "2. [Public API Overview](#Public-API-Overview)  \n",
    "3. [Tokenizers and Similarity Functions](#Tokenizers-and-Similarity-Functions)\n",
    "4. [Feature Creation](#Feature-Creation)\n",
    "5. [Featurization](#Featurization)\n",
    "6. [Down Sampling](#Down-Sampling)\n",
    "7. [Seed Creation](#Seed-Creation)\n",
    "8. [Training a Matcher](#Training-a-Matcher)\n",
    "9. [Applying a Matcher](#Applying-a-Matcher)\n",
    "10. [Active Learning Labeling](#Active-Learning-Labeling)\n",
    "11. [Custom Abstract Classes](#Custom-Abstract-Classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ccf19",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install MadLib and its dependencies:\n",
    "\n",
    "```bash\n",
    "pip install MadLib\n",
    "```\n",
    "\n",
    "Import all public API functions and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 14:41:37 WARN Utils: Your hostname, Devs-MacBook-Pro-3.local resolves to a loopback address: 127.0.0.1; using 192.168.1.96 instead (on interface en0)\n",
      "25/06/20 14:41:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/20 14:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from MadLib import (\n",
    "    # Core functions\n",
    "    get_base_tokenizers, get_extra_tokenizers, get_base_sim_functions,\n",
    "    create_features, featurize, down_sample,\n",
    "    create_seeds, train_matcher, apply_matcher, label_data,\n",
    "    # Abstract base classes for customization\n",
    "    Tokenizer, Vectorizer, Feature, MLModel, Labeler, CustomLabeler\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Spark - needed if you use label_data, featurize, a SparkML Model, or any Spark DataFrame\n",
    "spark = SparkSession.builder.appName('MadLibComprehensiveDemo').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab39e59",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Public API Overview\n",
    "\n",
    "MadMatcher exposes the following public API functions and abstract classes:\n",
    "\n",
    "### Core Functions\n",
    "- **`get_base_tokenizers()`**: Returns default tokenizers for text processing\n",
    "- **`get_extra_tokenizers()`**: Returns additional specialized tokenizers  \n",
    "- **`get_base_sim_functions()`**: Returns default similarity functions\n",
    "- **`create_features(A, B, a_cols, b_cols, tokenizers=None, sim_functions=None, null_threshold=0.5)`**: Generate feature objects for comparing records\n",
    "- **`featurize(features, A, B, candidates, output_col='features', fill_na=None)`**: Apply features to candidate pairs\n",
    "- **`down_sample(fvs, percent, search_id_column, score_column='score', bucket_size=1000)`**: Reduce dataset size by sampling\n",
    "- **`create_seeds(fvs, nseeds, labeler, score_column='score')`**: Generate initial labeled examples\n",
    "- **`train_matcher(model_spec, labeled_data, feature_col='features', label_col='label')`**: Train a matching model\n",
    "- **`apply_matcher(model, df, feature_col, output_col)`**: Apply trained model for predictions\n",
    "- **`label_data(model_spec, mode, labeler_spec, fvs, seeds=None)`**: Active learning for labeling\n",
    "\n",
    "\n",
    "### Abstract Base Classes (for customization)\n",
    "- **`Tokenizer`**: Base class for text tokenization strategies\n",
    "- **`Vectorizer`**: Base class for converting tokens to vectors  \n",
    "- **`Feature`**: Base class for similarity/distance features\n",
    "- **`MLModel`**: Base class for machine learning models\n",
    "- **`Labeler`**: Base class for labeling strategies\n",
    "- **`CustomLabeler`**: Extended labeler with access to full record data\n",
    "\n",
    "Each section below demonstrates these with different input types and configurations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980c867",
   "metadata": {},
   "source": [
    "## Tokenizers and Similarity Functions\n",
    "\n",
    "MadMatcher provides various tokenizers for text preprocessing and similarity functions for comparing records. Understanding these building blocks is essential for effective feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2158986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE TOKENIZERS:\n",
      "  1. StrippedWhiteSpaceTokenizer: stripped_whitespace_tokens\n",
      "  2. NumericTokenizer: num_tokens\n",
      "  3. QGramTokenizer: 3gram_tokens\n",
      "EXTRA TOKENIZERS:\n",
      "  1. AlphaNumericTokenizer: alnum_tokens\n",
      "  2. QGramTokenizer: 5gram_tokens\n",
      "  3. StrippedQGramTokenizer: stripped_3gram_tokens\n",
      "  4. StrippedQGramTokenizer: stripped_5gram_tokens\n",
      "BASE SIMILARITY FUNCTIONS:\n",
      "  1. TFIDFFeature\n",
      "  2. JaccardFeature\n",
      "  3. SIFFeature\n",
      "  4. OverlapCoeffFeature\n",
      "  5. CosineFeature\n"
     ]
    }
   ],
   "source": [
    "# Get available tokenizers and similarity functions\n",
    "base_tokenizers = get_base_tokenizers()\n",
    "extra_tokenizers = get_extra_tokenizers()\n",
    "base_sim_functions = get_base_sim_functions()\n",
    "\n",
    "print('BASE TOKENIZERS:')\n",
    "for i, tokenizer in enumerate(base_tokenizers):\n",
    "    print(f'  {i+1}. {tokenizer.__class__.__name__}: {tokenizer.NAME}')\n",
    "\n",
    "print('EXTRA TOKENIZERS:')\n",
    "for i, tokenizer in enumerate(extra_tokenizers):\n",
    "    print(f'  {i+1}. {tokenizer.__class__.__name__}: {tokenizer.NAME}')\n",
    "\n",
    "print('BASE SIMILARITY FUNCTIONS:')\n",
    "for i, sim_func in enumerate(base_sim_functions):\n",
    "    print(f'  {i+1}. {sim_func.__name__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16c2fb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Testing Tokenizers with Different Input Types\n",
    "\n",
    "Let's see how different tokenizers handle various types of input data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7f6431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZER TESTING ON DIFFERENT INPUT TYPES\n",
      "\n",
      "StrippedWhiteSpaceTokenizer (stripped_whitespace_tokens):\n",
      "  Input: 'Alice Smith'        → Tokens: ['alice', 'smith']\n",
      "  Input: 'Bob Jones Jr.'      → Tokens: ['bob', 'jones', 'jr']\n",
      "  Input: \"Jean-Pierre O'Connor\" → Tokens: ['jeanpierre', 'oconnor']\n",
      "  Input: 'Company123 Inc.'    → Tokens: ['company123', 'inc']\n",
      "  Input: '123-456-7890'       → Tokens: ['1234567890']\n",
      "  Input: ''                   → Tokens: []\n",
      "  Input: None                 → Tokens: None\n",
      "  Input: 42                   → Tokens: ['42']\n",
      "\n",
      "NumericTokenizer (num_tokens):\n",
      "  Input: 'Alice Smith'        → Tokens: []\n",
      "  Input: 'Bob Jones Jr.'      → Tokens: []\n",
      "  Input: \"Jean-Pierre O'Connor\" → Tokens: []\n",
      "  Input: 'Company123 Inc.'    → Tokens: ['123']\n",
      "  Input: '123-456-7890'       → Tokens: ['123', '456', '7890']\n",
      "  Input: ''                   → Tokens: []\n",
      "  Input: None                 → Tokens: None\n",
      "  Input: 42                   → Tokens: ['42']\n",
      "\n",
      "QGramTokenizer (3gram_tokens):\n",
      "  Input: 'Alice Smith'        → Tokens: ['ali', 'lic', 'ice', 'ce ', 'e s', ' sm', 'smi', 'mit', 'ith']\n",
      "  Input: 'Bob Jones Jr.'      → Tokens: ['bob', 'ob ', 'b j', ' jo', 'jon', 'one', 'nes', 'es ', 's j', ' jr', 'jr.']\n",
      "  Input: \"Jean-Pierre O'Connor\" → Tokens: ['jea', 'ean', 'an-', 'n-p', '-pi', 'pie', 'ier', 'err', 'rre', 're ', 'e o', \" o'\", \"o'c\", \"'co\", 'con', 'onn', 'nno', 'nor']\n",
      "  Input: 'Company123 Inc.'    → Tokens: ['com', 'omp', 'mpa', 'pan', 'any', 'ny1', 'y12', '123', '23 ', '3 i', ' in', 'inc', 'nc.']\n",
      "  Input: '123-456-7890'       → Tokens: ['123', '23-', '3-4', '-45', '456', '56-', '6-7', '-78', '789', '890']\n",
      "  Input: ''                   → Tokens: []\n",
      "  Input: None                 → Tokens: None\n",
      "  Input: 42                   → Tokens: []\n",
      "\n",
      "AlphaNumericTokenizer (alnum_tokens):\n",
      "  Input: 'Alice Smith'        → Tokens: ['alice', 'smith']\n",
      "  Input: 'Bob Jones Jr.'      → Tokens: ['bob', 'jones', 'jr']\n",
      "  Input: \"Jean-Pierre O'Connor\" → Tokens: ['jean', 'pierre', 'o', 'connor']\n",
      "  Input: 'Company123 Inc.'    → Tokens: ['company123', 'inc']\n",
      "  Input: '123-456-7890'       → Tokens: ['123', '456', '7890']\n",
      "  Input: ''                   → Tokens: []\n",
      "  Input: None                 → Tokens: None\n",
      "  Input: 42                   → Tokens: ['42']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test different input types with tokenizers\n",
    "test_inputs = [\n",
    "    'Alice Smith',           # Normal name\n",
    "    'Bob Jones Jr.',         # Name with suffix\n",
    "    'Jean-Pierre O\\'Connor', # Name with special characters\n",
    "    'Company123 Inc.',       # Mixed alphanumeric\n",
    "    '123-456-7890',          # Phone number\n",
    "    '',                      # Empty string\n",
    "    None,                    # None value\n",
    "    42                       # Numeric value\n",
    "]\n",
    "\n",
    "print(\"TOKENIZER TESTING ON DIFFERENT INPUT TYPES\\n\")\n",
    "\n",
    "# Test different tokenizers\n",
    "tokenizers_to_test = [\n",
    "    base_tokenizers[0],  # StrippedWhiteSpaceTokenizer\n",
    "    base_tokenizers[1],  # NumericTokenizer  \n",
    "    base_tokenizers[2],  # QGramTokenizer\n",
    "    extra_tokenizers[0]  # AlphaNumericTokenizer\n",
    "]\n",
    "\n",
    "for tokenizer in tokenizers_to_test:\n",
    "    print(f\"{tokenizer.__class__.__name__} ({tokenizer.NAME}):\")\n",
    "    for input_val in test_inputs:\n",
    "        try:\n",
    "            if input_val is None:\n",
    "                tokens = tokenizer.tokenize(input_val)\n",
    "            else:\n",
    "                tokens = tokenizer.tokenize(str(input_val))\n",
    "            print(f\"  Input: {repr(input_val):<20} → Tokens: {tokens}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Input: {repr(input_val):<20} → Error: {type(e).__name__}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a5c4d",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "\n",
    "Feature creation is the core of MadLib's functionality. The `create_features()` function automatically generates appropriate features based on your data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feea15f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE DATASETS:\n",
      "\n",
      "Table A:\n",
      " _id         name  age           email        phone     address\n",
      "   1  Alice Smith 25.0 alice@email.com 123-456-7890 123 Main St\n",
      "   2    Bob Jones 30.0   bob@email.com 987-654-3210 456 Oak Ave\n",
      "   3  Carol Davis 28.0            None 555-123-4567 789 Pine Rd\n",
      "   4 David Wilson  NaN david@email.com               321 Elm St\n",
      "\n",
      "Table B:\n",
      " _id           name  age            email        phone     address\n",
      " 101   Alicia Smith   26 alicia@email.com 123-456-7891 124 Main St\n",
      " 102   Robert Jones   29 robert@gmail.com 987-654-3211 457 Oak Ave\n",
      " 103 Caroline Davis   28  carol@email.com              790 Pine Rd\n",
      " 104    Dave Wilson   35             None 555-999-8888  322 Elm St\n"
     ]
    }
   ],
   "source": [
    "# Create more comprehensive toy DataFrames for testing\n",
    "A = pd.DataFrame({\n",
    "    '_id': [1, 2, 3, 4],\n",
    "    'name': ['Alice Smith', 'Bob Jones', 'Carol Davis', 'David Wilson'],\n",
    "    'age': [25, 30, 28, None],  # Include missing values\n",
    "    'email': ['alice@email.com', 'bob@email.com', None, 'david@email.com'],\n",
    "    'phone': ['123-456-7890', '987-654-3210', '555-123-4567', ''],\n",
    "    'address': ['123 Main St', '456 Oak Ave', '789 Pine Rd', '321 Elm St']\n",
    "})\n",
    "\n",
    "B = pd.DataFrame({\n",
    "    '_id': [101, 102, 103, 104], \n",
    "    'name': ['Alicia Smith', 'Robert Jones', 'Caroline Davis', 'Dave Wilson'],\n",
    "    'age': [26, 29, 28, 35],\n",
    "    'email': ['alicia@email.com', 'robert@gmail.com', 'carol@email.com', None],\n",
    "    'phone': ['123-456-7891', '987-654-3211', '', '555-999-8888'],\n",
    "    'address': ['124 Main St', '457 Oak Ave', '790 Pine Rd', '322 Elm St']\n",
    "})\n",
    "\n",
    "print(\"SAMPLE DATASETS:\")\n",
    "print(\"\\nTable A:\")\n",
    "print(A.to_string(index=False))\n",
    "print(\"\\nTable B:\")\n",
    "print(B.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc4976",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Different Feature Creation Configurations\n",
    "\n",
    "The `create_features()` function can be customized in several ways:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE CREATION CONFIGURATIONS:\n",
      "\n",
      "DEFAULT FEATURES (8 features):\n",
      "   1. exact_match(name, name)\n",
      "   2. exact_match(age, age)\n",
      "   3. rel_diff(age, age)\n",
      "   4. tf_idf_3gram_tokens(name, name)\n",
      "   5. jaccard(3gram_tokens(name), 3gram_tokens(name))\n",
      "   6. sif_3gram_tokens(name, name)\n",
      "   7. overlap_coeff(3gram_tokens(name), 3gram_tokens(name))\n",
      "   8. cosine(3gram_tokens(name), 3gram_tokens(name))\n",
      "\n",
      "NUMERIC-ONLY FEATURES (2 features):\n",
      "   1. exact_match(age, age)\n",
      "   2. rel_diff(age, age)\n",
      "\n",
      "CUSTOM TOKENIZERS & SIM FUNCTIONS (4 features):\n",
      "   1. exact_match(name, name)\n",
      "   2. monge_elkan_jw(name, name)\n",
      "   3. edit_distance(name, name)\n",
      "   4. smith_waterman(name, name)\n",
      "\n",
      "NULL-FILTERED FEATURES (14 features):\n",
      "   1. exact_match(name, name)\n",
      "   2. exact_match(age, age)\n",
      "   3. exact_match(email, email)\n",
      "   4. rel_diff(age, age)\n",
      "   5. tf_idf_3gram_tokens(name, name)\n",
      "   6. jaccard(3gram_tokens(name), 3gram_tokens(name))\n",
      "   7. sif_3gram_tokens(name, name)\n",
      "   8. overlap_coeff(3gram_tokens(name), 3gram_tokens(name))\n",
      "   9. cosine(3gram_tokens(name), 3gram_tokens(name))\n",
      "   10. tf_idf_3gram_tokens(email, email)\n",
      "   11. jaccard(3gram_tokens(email), 3gram_tokens(email))\n",
      "   12. sif_3gram_tokens(email, email)\n",
      "   13. overlap_coeff(3gram_tokens(email), 3gram_tokens(email))\n",
      "   14. cosine(3gram_tokens(email), 3gram_tokens(email))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"FEATURE CREATION CONFIGURATIONS:\\n\")\n",
    "\n",
    "# 1. Default configuration (automatic feature generation)\n",
    "features_default = create_features(A, B, a_cols=['name', 'age'], b_cols=['name', 'age'])\n",
    "print(f\"DEFAULT FEATURES ({len(features_default)} features):\")\n",
    "for i, feature in enumerate(features_default):\n",
    "    print(f\"   {i+1}. {feature}\")\n",
    "print()\n",
    "\n",
    "# 2. Only numeric columns (for RelDiff features)\n",
    "features_numeric = create_features(A, B, a_cols=['age'], b_cols=['age'])\n",
    "print(f\"NUMERIC-ONLY FEATURES ({len(features_numeric)} features):\")\n",
    "for i, feature in enumerate(features_numeric):\n",
    "    print(f\"   {i+1}. {feature}\")\n",
    "print()\n",
    "\n",
    "# 3. Custom tokenizers and similarity functions\n",
    "from MadLib._internal.tokenizer.tokenizer import AlphaNumericTokenizer\n",
    "from MadLib._internal.feature.token_feature import JaccardFeature, CosineFeature\n",
    "\n",
    "custom_tokenizers = [AlphaNumericTokenizer()]\n",
    "custom_sim_functions = [JaccardFeature, CosineFeature]\n",
    "\n",
    "features_custom = create_features(\n",
    "    A, B, \n",
    "    a_cols=['name'], b_cols=['name'],\n",
    "    tokenizers=custom_tokenizers,\n",
    "    sim_functions=custom_sim_functions\n",
    ")\n",
    "print(f\"CUSTOM TOKENIZERS & SIM FUNCTIONS ({len(features_custom)} features):\")\n",
    "for i, feature in enumerate(features_custom):\n",
    "    print(f\"   {i+1}. {feature}\")\n",
    "print()\n",
    "\n",
    "# 4. High null threshold (filters out columns with many missing values)\n",
    "features_filtered = create_features(\n",
    "    A, B, \n",
    "    a_cols=['name', 'age', 'email'], b_cols=['name', 'age', 'email'],\n",
    "    null_threshold=0.3  # Only use columns with <30% null values\n",
    ")\n",
    "print(f\"NULL-FILTERED FEATURES ({len(features_filtered)} features):\")\n",
    "for i, feature in enumerate(features_filtered):\n",
    "    print(f\"   {i+1}. {feature}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bab20",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Featurization\n",
    "\n",
    "Featurization applies the generated features to candidate record pairs, producing feature vectors for machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8af09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURIZATION EXAMPLES:\n",
      "\n",
      "CANDIDATE PAIR EXAMPLES:\n",
      "\n",
      "1. Basic pairs:\n",
      "  id1_list  id2\n",
      "0      [1]  101\n",
      "1      [2]  102\n",
      "2      [3]  103\n",
      "3      [4]  104\n",
      "\n",
      "2. Blocked pairs (one-to-many):\n",
      "  id1_list  id2\n",
      "0   [1, 2]  101\n",
      "1      [3]  103\n",
      "2      [4]  104\n",
      "\n",
      "3. Custom pairs with metadata:\n",
      "  id1_list  id2      block_id  similarity_score\n",
      "0      [1]  101  name_block_1               0.8\n",
      "1      [2]  102  name_block_2               0.6\n",
      "2      [3]  103  name_block_1               0.9\n"
     ]
    }
   ],
   "source": [
    "# Create different types of candidate pairs\n",
    "print(\"FEATURIZATION EXAMPLES:\\n\")\n",
    "\n",
    "# 1. Basic candidate pairs (one-to-one mapping)\n",
    "candidates_basic = pd.DataFrame({\n",
    "    'id1_list': [[1], [2], [3], [4]],  # Each record from A\n",
    "    'id2': [101, 102, 103, 104]        # Paired with record from B\n",
    "})\n",
    "\n",
    "# 2. One-to-many candidates (blocking results)\n",
    "candidates_blocked = pd.DataFrame({\n",
    "    'id1_list': [[1, 2], [3], [4]],    # Multiple A records can match one B record\n",
    "    'id2': [101, 103, 104]\n",
    "})\n",
    "\n",
    "# 3. Custom candidates with additional metadata\n",
    "candidates_custom = pd.DataFrame({\n",
    "    'id1_list': [[1], [2], [3]],\n",
    "    'id2': [101, 102, 103],\n",
    "    'block_id': ['name_block_1', 'name_block_2', 'name_block_1'],  # Additional info\n",
    "    'similarity_score': [0.8, 0.6, 0.9]  # Pre-computed scores\n",
    "})\n",
    "\n",
    "print(\"CANDIDATE PAIR EXAMPLES:\")\n",
    "print(\"\\n1. Basic pairs:\")\n",
    "print(candidates_basic)\n",
    "print(\"\\n2. Blocked pairs (one-to-many):\")  \n",
    "print(candidates_blocked)\n",
    "print(\"\\n3. Custom pairs with metadata:\")\n",
    "print(candidates_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d1aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FEATURIZATION RESULTS:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC FEATURIZATION:\n",
      " Shape: (4, 4)\n",
      " Columns: ['id2', 'id1', 'features', '_id']\n",
      " Feature vector length: 8\n",
      " Sample feature vector: [0.0, 0.0, 0.03846153989434242, 0.5474452376365662, 0.4615384638309479]... (showing first 5)\n",
      "\n",
      "CUSTOM OUTPUT COLUMN:\n",
      " Columns: ['id2', 'id1', 'similarity_features', '_id']\n",
      "\n",
      "WITH NaN FILLING:\n",
      " NaN count in features: 0\n",
      " NaN count after filling: 0\n",
      "\n",
      "SAMPLE FEATURIZATION OUTPUT:\n",
      "   id2  id1          _id\n",
      "0  101    1   8589934592\n",
      "1  102    2   8589934593\n",
      "2  104    4  17179869184\n",
      "3  103    3  25769803776\n",
      "\n",
      "Feature vectors (first 3 features for first 3 pairs):\n",
      " Pair 1: [0.0, 0.0, 0.03846153989434242]\n",
      " Pair 2: [0.0, 0.0, 0.03333333507180214]\n",
      " Pair 3: [0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Apply featurization with different configurations\n",
    "print(\"\\nFEATURIZATION RESULTS:\\n\")\n",
    "\n",
    "# Use the default features for comprehensive comparison\n",
    "features = features_default\n",
    "\n",
    "# 1. Basic featurization\n",
    "fvs_basic = featurize(features, A, B, candidates_basic)\n",
    "print(f\"BASIC FEATURIZATION:\")\n",
    "print(f\" Shape: {fvs_basic.shape}\")\n",
    "print(f\" Columns: {list(fvs_basic.columns)}\")\n",
    "print(f\" Feature vector length: {len(fvs_basic['features'].iloc[0])}\")\n",
    "print(f\" Sample feature vector: {fvs_basic['features'].iloc[0][:5]}... (showing first 5)\")\n",
    "print()\n",
    "\n",
    "# 2. Featurization with custom output column name\n",
    "fvs_custom_col = featurize(features, A, B, candidates_basic, output_col='similarity_features')\n",
    "print(f\"CUSTOM OUTPUT COLUMN:\")\n",
    "print(f\" Columns: {list(fvs_custom_col.columns)}\")\n",
    "print()\n",
    "\n",
    "# 3. Featurization with fill_na parameter\n",
    "fvs_filled = featurize(features, A, B, candidates_basic, fill_na=0.0)\n",
    "print(f\"WITH NaN FILLING:\")\n",
    "print(f\" NaN count in features: {sum(np.isnan(x).sum() for x in fvs_basic['features'])}\")\n",
    "print(f\" NaN count after filling: {sum(np.isnan(x).sum() for x in fvs_filled['features'])}\")\n",
    "print()\n",
    "\n",
    "# Display sample results\n",
    "print(\"SAMPLE FEATURIZATION OUTPUT:\")\n",
    "print(fvs_basic[['id2', 'id1', '_id']].head())\n",
    "print(\"\\nFeature vectors (first 3 features for first 3 pairs):\")\n",
    "for i in range(min(3, len(fvs_basic))):\n",
    "    print(f\" Pair {i+1}: {fvs_basic['features'].iloc[i][:3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c33a4",
   "metadata": {},
   "source": [
    "## Down Sampling\n",
    "\n",
    "Down sampling reduces the size of your feature vector dataset while preserving the most promising candidate pairs based on a scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5958d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWN SAMPLING EXAMPLES:\n",
      "\n",
      "Original dataset size: 4 pairs\n",
      "Original scores: [0.354, 0.249, 0.416, 0.16]\n",
      "\n",
      "50% DOWN SAMPLING:\n",
      " Result size: 2 pairs\n",
      " Retained scores: [0.354, 0.416]\n",
      "\n",
      "25% DOWN SAMPLING:\n",
      " Result size: 1 pairs\n",
      " Retained scores: [0.416]\n",
      "\n",
      "CUSTOM SCORE COLUMN:\n",
      " Result size: 2 pairs\n",
      " Retained random_scores: [0.525, 0.432]\n",
      "\n",
      "CUSTOM BUCKET SIZE:\n",
      " Result size: 2 pairs\n",
      " Bucket size parameter: 2 (for demonstration)\n",
      "\n",
      "Note: Down sampling preserves the highest-scoring pairs within each hash bucket.\n"
     ]
    }
   ],
   "source": [
    "# First, let's work with our basic feature vectors and add scores\n",
    "fvs = fvs_basic.copy()\n",
    "\n",
    "# Add various types of scores for demonstration\n",
    "np.random.seed(42)  # For reproducible results\n",
    "fvs['score'] = np.random.beta(2, 5, len(fvs))  # Simulated similarity scores\n",
    "fvs['random_score'] = np.random.uniform(0, 1, len(fvs))\n",
    "\n",
    "print(\"DOWN SAMPLING EXAMPLES:\\n\")\n",
    "print(f\"Original dataset size: {len(fvs)} pairs\")\n",
    "print(\"Original scores:\", fvs['score'].round(3).tolist())\n",
    "print()\n",
    "\n",
    "# 1. Basic down sampling (50%)\n",
    "down_50 = down_sample(fvs, percent=0.5, search_id_column='id2')\n",
    "print(f\"50% DOWN SAMPLING:\")\n",
    "print(f\" Result size: {len(down_50)} pairs\")\n",
    "print(f\" Retained scores: {down_50['score'].round(3).tolist()}\")\n",
    "print()\n",
    "\n",
    "# 2. Aggressive down sampling (25%)  \n",
    "down_25 = down_sample(fvs, percent=0.25, search_id_column='id2')\n",
    "print(f\"25% DOWN SAMPLING:\")\n",
    "print(f\" Result size: {len(down_25)} pairs\")\n",
    "print(f\" Retained scores: {down_25['score'].round(3).tolist()}\")\n",
    "print()\n",
    "\n",
    "# 3. Custom score column\n",
    "down_custom = down_sample(fvs, percent=0.5, search_id_column='id2', score_column='random_score')\n",
    "print(f\"CUSTOM SCORE COLUMN:\")\n",
    "print(f\" Result size: {len(down_custom)} pairs\")\n",
    "print(f\" Retained random_scores: {down_custom['random_score'].round(3).tolist()}\")\n",
    "print()\n",
    "\n",
    "# 4. Custom bucket size for large datasets\n",
    "down_buckets = down_sample(fvs, percent=0.75, search_id_column='id2', bucket_size=2)\n",
    "print(f\"CUSTOM BUCKET SIZE:\")\n",
    "print(f\" Result size: {len(down_buckets)} pairs\")\n",
    "print(f\" Bucket size parameter: 2 (for demonstration)\")\n",
    "print()\n",
    "\n",
    "print(\"Note: Down sampling preserves the highest-scoring pairs within each hash bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a2d5e",
   "metadata": {},
   "source": [
    "## Seed Creation\n",
    "\n",
    "Seeds are initial labeled examples used to train machine learning models. MadLib supports various labeling strategies for creating these seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2bc5657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED CREATION EXAMPLES:\n",
      "\n",
      "Ground truth matches:\n",
      "   id1  id2\n",
      "0    1  101\n",
      "1    3  103\n",
      "2    4  104\n",
      "\n",
      "DIFFERENT LABELING STRATEGIES:\n",
      "\n",
      "seeds: pos_count = 2 neg_count = 1\n",
      "GOLD STANDARD LABELER:\n",
      " Seeds created: 3\n",
      " Positive labels: 2\n",
      " Negative labels: 1\n",
      "  Sample seeds:\n",
      " id1  id2    score  label\n",
      "   4  104 0.415959    1.0\n",
      "   3  103 0.159968    1.0\n",
      "   2  102 0.248558    0.0\n",
      "\n",
      "seeds: pos_count = 3 neg_count = 0\n",
      "RULE-BASED LABELER:\n",
      " Seeds created: 3\n",
      " Positive labels: 3\n",
      " Negative labels: 0\n",
      "  Sample seeds:\n",
      " id1  id2    score  label\n",
      "   4  104 0.415959    1.0\n",
      "   3  103 0.159968    1.0\n",
      "   2  102 0.248558    1.0\n",
      "\n",
      "seeds: pos_count = 2 neg_count = 0\n",
      "ALWAYS-POSITIVE LABELER:\n",
      " Seeds created: 2\n",
      " All labels: [1.0, 1.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SEED CREATION EXAMPLES:\\n\")\n",
    "\n",
    "# Create different types of labelers for demonstration\n",
    "\n",
    "# 1. Gold standard labeler (ground truth)\n",
    "gold_labels = pd.DataFrame({\n",
    "    'id1': [1, 3, 4],        # Records from table A\n",
    "    'id2': [101, 103, 104]   # Matching records from table B  \n",
    "})\n",
    "print(\"Ground truth matches:\")\n",
    "print(gold_labels)\n",
    "print()\n",
    "\n",
    "gold_labeler = {'name': 'gold', 'gold': gold_labels}\n",
    "\n",
    "# 2. Create a custom always-positive labeler\n",
    "class AlwaysPositiveLabeler(Labeler):\n",
    "    def __call__(self, id1, id2):\n",
    "        return 1.0  # Always return positive match\n",
    "\n",
    "# 3. Create a custom rule-based labeler  \n",
    "class RuleBasedLabeler(Labeler):\n",
    "    def __call__(self, id1, id2):\n",
    "        # Simple rule: match if id2 - id1 == 100\n",
    "        return 1.0 if (id2 - id1) == 100 else 0.0\n",
    "\n",
    "print(\"DIFFERENT LABELING STRATEGIES:\\n\")\n",
    "\n",
    "# Test with gold labeler\n",
    "seeds_gold = create_seeds(fvs, nseeds=3, labeler=gold_labeler)\n",
    "print(\"GOLD STANDARD LABELER:\")\n",
    "print(f\" Seeds created: {len(seeds_gold)}\")\n",
    "print(f\" Positive labels: {sum(seeds_gold['label'] == 1.0)}\")\n",
    "print(f\" Negative labels: {sum(seeds_gold['label'] == 0.0)}\")\n",
    "print(\"  Sample seeds:\")\n",
    "print(seeds_gold[['id1', 'id2', 'score', 'label']].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Test with rule-based labeler\n",
    "rule_labeler = RuleBasedLabeler()\n",
    "seeds_rule = create_seeds(fvs, nseeds=3, labeler=rule_labeler)\n",
    "print(\"RULE-BASED LABELER:\")\n",
    "print(f\" Seeds created: {len(seeds_rule)}\")\n",
    "print(f\" Positive labels: {sum(seeds_rule['label'] == 1.0)}\")\n",
    "print(f\" Negative labels: {sum(seeds_rule['label'] == 0.0)}\")\n",
    "print(\"  Sample seeds:\")\n",
    "print(seeds_rule[['id1', 'id2', 'score', 'label']].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Test with always-positive labeler\n",
    "positive_labeler = AlwaysPositiveLabeler()\n",
    "seeds_positive = create_seeds(fvs, nseeds=2, labeler=positive_labeler)\n",
    "print(\"ALWAYS-POSITIVE LABELER:\")\n",
    "print(f\" Seeds created: {len(seeds_positive)}\")\n",
    "print(f\" All labels: {seeds_positive['label'].tolist()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf3c22",
   "metadata": {},
   "source": [
    "## Training a Matcher\n",
    "\n",
    "MadLib supports various machine learning models for training matchers, including scikit-learn models and custom implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bee85b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DIFFERENT TYPES OF MATCHERS:\n",
      "\n",
      "Training data: 3 labeled examples\n",
      "Feature vector dimension: 8\n",
      "\n",
      "LOGISTIC REGRESSION MODEL:\n",
      " Model type: LogisticRegression\n",
      " Parameters: {'model': \"<class 'sklearn.linear_model._logistic.LogisticRegression'>\", 'nan_fill': 0, 'model_args': {'random_state': 42}}\n",
      "\n",
      "RANDOM FOREST MODEL:\n",
      " Model type: RandomForestClassifier\n",
      " Parameters: {'model': \"<class 'sklearn.ensemble._forest.RandomForestClassifier'>\", 'nan_fill': 0, 'model_args': {'n_estimators': 10, 'random_state': 42}}\n",
      "\n",
      "CUSTOM THRESHOLD MODEL:\n",
      " Model type: SimpleThresholdModel\n",
      " Parameters: {'threshold': 0.1, 'trained': True}\n",
      "\n",
      "All models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING DIFFERENT TYPES OF MATCHERS:\\n\")\n",
    "\n",
    "# Use the gold standard seeds for training\n",
    "seeds = seeds_gold\n",
    "labeled_data = seeds.copy()\n",
    "\n",
    "print(f\"Training data: {len(labeled_data)} labeled examples\")\n",
    "print(f\"Feature vector dimension: {len(labeled_data['features'].iloc[0])}\")\n",
    "print()\n",
    "\n",
    "# 1. Logistic Regression\n",
    "model_lr = train_matcher(\n",
    "    {'model_type': 'sklearn', 'model': LogisticRegression, 'nan_fill': 0, 'model_args': {'random_state': 42}}, \n",
    "    labeled_data\n",
    ")\n",
    "print(\"LOGISTIC REGRESSION MODEL:\")\n",
    "print(f\" Model type: {type(model_lr.trained_model).__name__}\")\n",
    "print(f\" Parameters: {model_lr.params_dict()}\")\n",
    "print()\n",
    "\n",
    "# 2. Random Forest\n",
    "model_rf = train_matcher(\n",
    "    {'model_type': 'sklearn', 'model': RandomForestClassifier, 'nan_fill': 0,\n",
    "     'model_args': {'n_estimators': 10, 'random_state': 42}}, \n",
    "    labeled_data\n",
    ")\n",
    "print(\"RANDOM FOREST MODEL:\")\n",
    "print(f\" Model type: {type(model_rf.trained_model).__name__}\")\n",
    "print(f\" Parameters: {model_rf.params_dict()}\")\n",
    "print()\n",
    "\n",
    "# 3. Custom MLModel implementation\n",
    "class SimpleThresholdModel(MLModel):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "        self._trained = False\n",
    "        self._trained_model = None\n",
    "        \n",
    "    @property\n",
    "    def nan_fill(self): return 0.0\n",
    "    @property  \n",
    "    def use_vectors(self): return False\n",
    "    @property\n",
    "    def use_floats(self): return True\n",
    "\n",
    "    def trained_model(self):\n",
    "        return self._trained_model\n",
    "        \n",
    "    def train(self, df, vector_col, label_column, return_estimator=False):\n",
    "        self._trained = True\n",
    "        self._trained_model = self\n",
    "        return self\n",
    "        \n",
    "    def predict(self, df, vector_col, output_col):\n",
    "        # Simple rule: predict 1 if first feature > threshold\n",
    "        df = df.copy()\n",
    "        df[output_col] = df[vector_col].apply(lambda x: 1.0 if len(x) > 0 and x[0] > self.threshold else 0.0)\n",
    "        return df\n",
    "        \n",
    "    def prediction_conf(self, df, vector_col, label_column):\n",
    "        df = df.copy()\n",
    "        df['conf'] = 0.8  # Fixed confidence\n",
    "        return df\n",
    "        \n",
    "    def entropy(self, df, vector_col, output_col):\n",
    "        df = df.copy()\n",
    "        df[output_col] = 0.5  # Fixed entropy\n",
    "        return df\n",
    "        \n",
    "    def params_dict(self):\n",
    "        return {'threshold': self.threshold, 'trained': self._trained}\n",
    "\n",
    "custom_model = SimpleThresholdModel(threshold=0.1)\n",
    "model_custom = train_matcher(custom_model, labeled_data)\n",
    "print(\"CUSTOM THRESHOLD MODEL:\")\n",
    "print(f\" Model type: {type(model_custom).__name__}\")\n",
    "print(f\" Parameters: {model_custom.params_dict()}\")\n",
    "print()\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0f91a",
   "metadata": {},
   "source": [
    "## Applying a Matcher\n",
    "\n",
    "Once trained, matchers can be applied to new data to generate predictions and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d5cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLYING TRAINED MATCHERS:\n",
      "\n",
      "LOGISTIC REGRESSION PREDICTIONS:\n",
      "   id1  id2     score  lr_prediction\n",
      "0    1  101  0.353677            1.0\n",
      "1    2  102  0.248558            1.0\n",
      "2    4  104  0.415959            1.0\n",
      "3    3  103  0.159968            1.0\n",
      " Positive predictions: 4/4\n",
      "\n",
      "RANDOM FOREST PREDICTIONS:\n",
      "   id1  id2     score  rf_prediction\n",
      "0    1  101  0.353677            1.0\n",
      "1    2  102  0.248558            0.0\n",
      "2    4  104  0.415959            1.0\n",
      "3    3  103  0.159968            1.0\n",
      " Positive predictions: 3/4\n",
      "\n",
      "CUSTOM THRESHOLD PREDICTIONS:\n",
      "   id1  id2     score  custom_prediction\n",
      "0    1  101  0.353677                0.0\n",
      "1    2  102  0.248558                0.0\n",
      "2    4  104  0.415959                0.0\n",
      "3    3  103  0.159968                0.0\n",
      " Positive predictions: 0/4\n",
      "\n",
      "PREDICTION COMPARISON:\n",
      " id1  id2  original_score  lr_pred  rf_pred  custom_pred\n",
      "   1  101           0.354      1.0      1.0          0.0\n",
      "   2  102           0.249      1.0      0.0          0.0\n",
      "   4  104           0.416      1.0      1.0          0.0\n",
      "   3  103           0.160      1.0      1.0          0.0\n",
      "\n",
      "Model Agreement:\n",
      " LR vs RF: 75.00%\n",
      " LR vs Custom: 0.00%\n",
      " RF vs Custom: 25.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"APPLYING TRAINED MATCHERS:\\n\")\n",
    "\n",
    "# Apply different trained models to the same feature vectors\n",
    "test_fvs = fvs.copy()\n",
    "\n",
    "# 1. Apply Logistic Regression model\n",
    "result_lr = apply_matcher(model_lr, test_fvs, feature_col='features', output_col='lr_prediction')\n",
    "print(\"LOGISTIC REGRESSION PREDICTIONS:\")\n",
    "print(result_lr[['id1', 'id2', 'score', 'lr_prediction']].head())\n",
    "print(f\" Positive predictions: {sum(result_lr['lr_prediction'] == 1.0)}/{len(result_lr)}\")\n",
    "print()\n",
    "\n",
    "# 2. Apply Random Forest model\n",
    "result_rf = apply_matcher(model_rf, test_fvs, feature_col='features', output_col='rf_prediction')\n",
    "print(\"RANDOM FOREST PREDICTIONS:\")\n",
    "print(result_rf[['id1', 'id2', 'score', 'rf_prediction']].head())\n",
    "print(f\" Positive predictions: {sum(result_rf['rf_prediction'] == 1.0)}/{len(result_rf)}\")\n",
    "print()\n",
    "\n",
    "# 3. Apply Custom Threshold model\n",
    "result_custom = apply_matcher(model_custom, test_fvs, feature_col='features', output_col='custom_prediction')\n",
    "print(\"CUSTOM THRESHOLD PREDICTIONS:\")\n",
    "print(result_custom[['id1', 'id2', 'score', 'custom_prediction']].head())\n",
    "print(f\" Positive predictions: {sum(result_custom['custom_prediction'] == 1.0)}/{len(result_custom)}\")\n",
    "print()\n",
    "\n",
    "# 4. Compare all predictions\n",
    "comparison = pd.DataFrame({\n",
    "    'id1': result_lr['id1'],\n",
    "    'id2': result_lr['id2'],\n",
    "    'original_score': result_lr['score'].round(3),\n",
    "    'lr_pred': result_lr['lr_prediction'],\n",
    "    'rf_pred': result_rf['rf_prediction'],\n",
    "    'custom_pred': result_custom['custom_prediction']\n",
    "})\n",
    "\n",
    "print(\"PREDICTION COMPARISON:\")\n",
    "print(comparison.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Calculate agreement between models\n",
    "lr_rf_agreement = sum(result_lr['lr_prediction'] == result_rf['rf_prediction']) / len(result_lr)\n",
    "print(f\"Model Agreement:\")\n",
    "print(f\" LR vs RF: {lr_rf_agreement:.2%}\")\n",
    "print(f\" LR vs Custom: {sum(result_lr['lr_prediction'] == result_custom['custom_prediction']) / len(result_lr):.2%}\")\n",
    "print(f\" RF vs Custom: {sum(result_rf['rf_prediction'] == result_custom['custom_prediction']) / len(result_rf):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defadd4",
   "metadata": {},
   "source": [
    "## Active Learning Labeling\n",
    "\n",
    "\n",
    "Active learning helps efficiently label data by selecting the most informative examples for human review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4991cb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIVE LEARNING LABELING:\n",
      "\n",
      "BATCH MODE ACTIVE LEARNING:\n",
      "Batch mode waits for a batch of examples to be labeled before training a new model.\n",
      "Ran out of examples before reaching nseeds\n",
      "seeds: pos_count = 2 neg_count = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ent_active_learner.py:120 - train() ] 2025-06-20 14:42:10,059 : running al to completion would label everything, but self._terminate_if_label_everything is False so AL will still run\n",
      "[ent_active_learner.py:138 - train() ] 2025-06-20 14:42:10,103 : max iter = 4\n",
      "[ent_active_learner.py:142 - train() ] 2025-06-20 14:42:10,104 : starting iteration 0\n",
      "[ent_active_learner.py:144 - train() ] 2025-06-20 14:42:10,104 : training model\n",
      "[ent_active_learner.py:152 - train() ] 2025-06-20 14:42:10,285 : selecting and labeling new examples\n",
      "[ent_active_learner.py:186 - train() ] 2025-06-20 14:42:10,683 : new batch positive = 1.0 negative = 0.0, total positive = 3.0 negative = 1.0\n",
      "[ent_active_learner.py:189 - train() ] 2025-06-20 14:42:10,685 : all fvs labeled, terminating active learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batch mode completed: 4 examples labeled\n",
      "  - Positive matches: 3\n",
      "  - Negative matches: 1\n",
      "\n",
      "CONTINUOUS MODE ACTIVE LEARNING:\n",
      "Continuous mode trains new models as examples are labeled.\n",
      "Ran out of examples before reaching nseeds\n",
      "seeds: pos_count = 2 neg_count = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[cont_entropy_active_learner.py:177 - _training_loop() ] 2025-06-20 14:42:11,896 : Insufficient examples for active learning: 4 total, 3 seeds. No unlabeled examples to select from. Labeling all examples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Continuous mode completed: 4 examples labeled\n",
      "  - Positive matches: 3\n",
      "  - Negative matches: 1\n",
      "\n",
      "ACTIVE LEARNING INSIGHTS:\n",
      "• Both modes automatically stop when all available examples are labeled\n",
      "• With small datasets (like this 4-record example), both modes will label all examples\n",
      "• In larger datasets, active learning focuses on the most uncertain/informative examples\n",
      "• The 'Ran out of examples' message simply indicates the algorithm has processed all available data\n",
      "\n",
      "LABELED RESULTS COMPARISON:\n",
      "Batch mode results:\n",
      " id1  id2  label\n",
      "   4  104    1.0\n",
      "   3  103    1.0\n",
      "   2  102    0.0\n",
      "   1  101    1.0\n",
      "\n",
      "Continuous mode results:\n",
      " id1  id2  label\n",
      "   4  104    1.0\n",
      "   3  103    1.0\n",
      "   1  101    1.0\n",
      "   2  102    0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"ACTIVE LEARNING LABELING:\\n\")\n",
    "\n",
    "print(\"BATCH MODE ACTIVE LEARNING:\")\n",
    "print(\"Batch mode waits for a batch of examples to be labeled before training a new model.\")\n",
    "labels_batch = label_data(\n",
    "    model_spec={'model_type': 'sklearn', 'model': LogisticRegression, 'nan_fill': 0.0, 'model_args': {'random_state': 42}},\n",
    "    mode='batch',\n",
    "    labeler_spec=gold_labeler,\n",
    "    fvs=fvs\n",
    ")\n",
    "print(f\"✓ Batch mode completed: {len(labels_batch)} examples labeled\")\n",
    "print(f\"  - Positive matches: {sum(labels_batch['label'] == 1.0)}\")\n",
    "print(f\"  - Negative matches: {sum(labels_batch['label'] == 0.0)}\")\n",
    "print()\n",
    "\n",
    "print(\"CONTINUOUS MODE ACTIVE LEARNING:\")\n",
    "print(\"Continuous mode trains new models as examples are labeled.\")\n",
    "labels_continuous = label_data(\n",
    "    model_spec={'model_type': 'sklearn', 'model': LogisticRegression, 'nan_fill': 0.0, 'model_args': {'random_state': 42}},\n",
    "    mode='continuous',\n",
    "    labeler_spec=gold_labeler,\n",
    "    fvs=fvs,\n",
    ")\n",
    "print(f\"✓ Continuous mode completed: {len(labels_continuous)} examples labeled\")\n",
    "print(f\"  - Positive matches: {sum(labels_continuous['label'] == 1.0)}\")\n",
    "print(f\"  - Negative matches: {sum(labels_continuous['label'] == 0.0)}\")\n",
    "print()\n",
    "\n",
    "print(\"ACTIVE LEARNING INSIGHTS:\")\n",
    "print(\"• Both modes automatically stop when all available examples are labeled\")\n",
    "print(\"• With small datasets (like this 4-record example), both modes will label all examples\")\n",
    "print(\"• In larger datasets, active learning focuses on the most uncertain/informative examples\")\n",
    "print(\"• The 'Ran out of examples' message simply indicates the algorithm has processed all available data\")\n",
    "print()\n",
    "\n",
    "print(\"LABELED RESULTS COMPARISON:\")\n",
    "print(\"Batch mode results:\")\n",
    "print(labels_batch[['id1', 'id2', 'label']].to_string(index=False))\n",
    "print()\n",
    "print(\"Continuous mode results:\")\n",
    "print(labels_continuous[['id1', 'id2', 'label']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee45ec0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Custom Abstract Classes\n",
    "\n",
    "MadLib also supports customization for your specific use case. You can implement custom tokenizers, features, ML models, and labelers by extending the abstract base classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad4ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM IMPLEMENTATION EXAMPLES:\n",
      "\n",
      "CUSTOM TOKENIZER (ReverseWordTokenizer):\n",
      " Input: 'Alice Smith'\n",
      " Tokens: ['ecila', 'htims']\n",
      " Name: reverse_word_tokens\n",
      "\n",
      "CUSTOM FEATURE (WordCountDifferenceFeature):\n",
      " Feature string: word_count_diff(name, name)\n",
      " Input B: 'Alice Smith' (2 words)\n",
      " Input A values: ['Bob Jones', \"Jean-Pierre O'Connor\", 'X']\n",
      " Word count differences: [0.0, 0.0, 1.0]\n",
      "\n",
      "CUSTOM LABELER (SmartSimilarityLabeler):\n",
      " Logic: High name overlap + small age difference = match\n",
      " Pair (1,101): 'Alice Smith' vs 'Alicia Smith' → 0.5\n",
      " Pair (2,102): 'Bob Jones' vs 'Robert Jones' → 0.5\n",
      " Pair (3,103): 'Carol Davis' vs 'Caroline Davis' → 0.5\n",
      " Pair (4,104): 'David Wilson' vs 'Dave Wilson' → 0.5\n",
      "\n",
      "USING CUSTOM COMPONENTS IN PIPELINE:\n",
      " Created 2 features (including custom)\n",
      "   1. exact_match(name, name)\n",
      "   2. word_count_diff(name, name)\n",
      "\n",
      " Custom featurization result shape: (2, 4)\n",
      " Feature vector length: 2\n",
      "  Custom components integrated successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"CUSTOM IMPLEMENTATION EXAMPLES:\\n\")\n",
    "\n",
    "# 1. Custom Tokenizer\n",
    "class ReverseWordTokenizer(Tokenizer):\n",
    "    \"\"\"Tokenizer that reverses each word before returning tokens\"\"\"\n",
    "    NAME = 'reverse_word_tokens'\n",
    "    \n",
    "    def tokenize(self, s):\n",
    "        if not isinstance(s, str):\n",
    "            return None\n",
    "        words = s.lower().split()\n",
    "        return [word[::-1] for word in words]  # Reverse each word\n",
    "\n",
    "# Test custom tokenizer\n",
    "reverse_tokenizer = ReverseWordTokenizer()\n",
    "print(\"CUSTOM TOKENIZER (ReverseWordTokenizer):\")\n",
    "test_string = \"Alice Smith\"\n",
    "print(f\" Input: '{test_string}'\")\n",
    "print(f\" Tokens: {reverse_tokenizer.tokenize(test_string)}\")\n",
    "print(f\" Name: {reverse_tokenizer.NAME}\")\n",
    "print()\n",
    "\n",
    "# 2. Custom Feature\n",
    "class WordCountDifferenceFeature(Feature):\n",
    "    \"\"\"Feature that computes absolute difference in word count\"\"\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'word_count_diff({self.a_attr}, {self.b_attr})'\n",
    "    \n",
    "    def __call__(self, rec, recs):\n",
    "        b_value = rec[self.b_attr]\n",
    "        a_values = recs[self.a_attr]\n",
    "        \n",
    "        if not isinstance(b_value, str):\n",
    "            return pd.Series(np.nan, index=a_values.index)\n",
    "        \n",
    "        b_word_count = len(b_value.split()) if b_value else 0\n",
    "        \n",
    "        def word_count_diff(a_value):\n",
    "            if not isinstance(a_value, str):\n",
    "                return np.nan\n",
    "            a_word_count = len(a_value.split()) if a_value else 0\n",
    "            return abs(a_word_count - b_word_count)\n",
    "        \n",
    "        return a_values.apply(word_count_diff).astype(np.float64)\n",
    "    \n",
    "    def _preprocess(self, data, input_col):\n",
    "        return data  # No preprocessing needed\n",
    "    \n",
    "    def _preprocess_output_column(self, attr):\n",
    "        return None  # No preprocessing output\n",
    "\n",
    "# Test custom feature\n",
    "custom_feature = WordCountDifferenceFeature('name', 'name')\n",
    "print(\"CUSTOM FEATURE (WordCountDifferenceFeature):\")\n",
    "print(f\" Feature string: {custom_feature}\")\n",
    "\n",
    "# Create test data for feature\n",
    "test_rec = {'name': 'Alice Smith'}\n",
    "test_recs = pd.DataFrame({'name': ['Bob Jones', 'Jean-Pierre O\\'Connor', 'X']})\n",
    "feature_result = custom_feature(test_rec, test_recs)\n",
    "print(f\" Input B: '{test_rec['name']}' (2 words)\")\n",
    "print(f\" Input A values: {test_recs['name'].tolist()}\")\n",
    "print(f\" Word count differences: {feature_result.tolist()}\")\n",
    "print()\n",
    "\n",
    "# 3. Custom Labeler with complex logic\n",
    "class SmartSimilarityLabeler(CustomLabeler):\n",
    "    \"\"\"Labeler that uses multiple criteria for matching\"\"\"\n",
    "    \n",
    "    def label_pair(self, row1, row2):\n",
    "        # Get name similarity (simple word overlap)\n",
    "        name1_words = set(row1['name'].lower().split())\n",
    "        name2_words = set(row2['name'].lower().split())\n",
    "        name_overlap = len(name1_words & name2_words) / max(len(name1_words | name2_words), 1)\n",
    "        \n",
    "        # Get age similarity \n",
    "        age1, age2 = row1.get('age'), row2.get('age')\n",
    "        age_diff = abs(age1 - age2) if (age1 is not None and age2 is not None) else float('inf')\n",
    "        \n",
    "        # Labeling logic\n",
    "        if name_overlap >= 0.5 and age_diff <= 5:\n",
    "            return 1.0  # Strong match\n",
    "        elif name_overlap >= 0.3 or age_diff <= 2:\n",
    "            return 0.5  # Uncertain (could be treated as unsure)\n",
    "        else:\n",
    "            return 0.0  # No match\n",
    "\n",
    "smart_labeler = SmartSimilarityLabeler(A, B)\n",
    "print(\"CUSTOM LABELER (SmartSimilarityLabeler):\")\n",
    "print(\" Logic: High name overlap + small age difference = match\")\n",
    "\n",
    "# Test the smart labeler\n",
    "test_pairs = [(1, 101), (2, 102), (3, 103), (4, 104)]\n",
    "for id1, id2 in test_pairs:\n",
    "    label = smart_labeler(id1, id2)\n",
    "    row1 = A[A['_id'] == id1].iloc[0]\n",
    "    row2 = B[B['_id'] == id2].iloc[0]\n",
    "    print(f\" Pair ({id1},{id2}): '{row1['name']}' vs '{row2['name']}' → {label}\")\n",
    "print()\n",
    "\n",
    "# 4. Demonstrate custom implementations in pipeline\n",
    "print(\"USING CUSTOM COMPONENTS IN PIPELINE:\")\n",
    "\n",
    "# Create features with custom tokenizer\n",
    "custom_features = create_features(\n",
    "    A, B, \n",
    "    a_cols=['name'], b_cols=['name'],\n",
    "    tokenizers=[reverse_tokenizer],\n",
    "    sim_functions=get_base_sim_functions()[:2]  # Use first 2 similarity functions\n",
    ")\n",
    "\n",
    "# Add our custom feature\n",
    "custom_features.append(custom_feature)\n",
    "\n",
    "print(f\" Created {len(custom_features)} features (including custom)\")\n",
    "for i, feature in enumerate(custom_features):\n",
    "    print(f\"   {i+1}. {feature}\")\n",
    "print()\n",
    "\n",
    "# Test featurization with custom features\n",
    "small_candidates = pd.DataFrame({'id1_list': [[1], [2]], 'id2': [101, 102]})\n",
    "custom_fvs = featurize(custom_features, A, B, small_candidates)\n",
    "\n",
    "print(f\" Custom featurization result shape: {custom_fvs.shape}\")\n",
    "print(f\" Feature vector length: {len(custom_fvs['features'].iloc[0])}\")\n",
    "print(\"  Custom components integrated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bd99273-97d5-4de7-a409-b2343b7fc625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped. Notebook complete!\n"
     ]
    }
   ],
   "source": [
    "# Clean up Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Notebook complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7f0a0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This comprehensive notebook has demonstrated all the key functionality of MadLib:\n",
    "\n",
    "### Core Functions Covered\n",
    "**Tokenizers & Similarity Functions**: Understanding the building blocks  \n",
    "**Feature Creation**: Automatic and custom feature generation  \n",
    "**Featurization**: Converting candidate pairs to feature vectors  \n",
    "**Down Sampling**: Intelligent dataset reduction  \n",
    "**Seed Creation**: Generating initial training labels  \n",
    "**Model Training**: Multiple ML approaches  \n",
    "**Model Application**: Making predictions  \n",
    "**Active Learning**: Efficient labeling strategies  \n",
    "\n",
    "### Abstract Classes Extended\n",
    "**Custom Tokenizer**: `ReverseWordTokenizer`  \n",
    "**Custom Feature**: `WordCountDifferenceFeature`  \n",
    "**Custom MLModel**: `SimpleThresholdModel`  \n",
    "**Custom Labeler**: `SmartSimilarityLabeler`  \n",
    "\n",
    "\n",
    "\n",
    "For more information, visit the [MadMatcher Website](https://madmatcher.ai) or explore the source code in the `MadLib` package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkly",
   "language": "python",
   "name": "sparkly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
